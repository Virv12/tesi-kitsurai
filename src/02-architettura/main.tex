\section{Architettura}
\label{sec:architettura}

Un sistema Kitsurai opera su un cluster, composto da un insieme statico di nodi, i quali possono risultare temporaneamente non disponibili.
Il sistema organizza i dati in tabelle, ognuna identificata da un UUIDv7. Ogni tabella contiene coppie chiave-valore, dove sia le chiavi che i valori sono array di byte.

\subsection{Interfaccia}
\label{subsec:interfaccia}

Sono supportate quattro operazioni principali:
\begin{itemize}
    \item \texttt{create-table}: crea una nuova tabella con i parametri specificati.
    \item \texttt{delete-table}: cancella una tabella esistente.
    \item \texttt{get}: recupera il valore associato a una chiave in una tabella.
    \item \texttt{set}: imposta il valore associato a una chiave in una tabella.
\end{itemize}

\subsection{Tabelle}
\label{subsec:tabelle}

Ogni tabella ha quattro parametri, specificati dall'utente al momento della creazione:
\begin{itemize}
    \item \texttt{bandwidth} o \texttt{b}: il numero di richieste al secondo che il sistema deve garantire per la tabella.
    \item \texttt{n}: il fattore di replicazione per ogni coppia chiave-valore.
    \item \texttt{r}: il \texttt{read-concern}, ovvero il numero minimo di nodi che devono rispondere con successo a una richiesta di lettura.
    \item \texttt{w}: il \texttt{write-concern}, ovvero il numero minimo di nodi che devono rispondere con successo a una richiesta di scrittura.
\end{itemize}

Ogni tabella è gestita da un insieme di nodi, chiamati \texttt{nodi responsabili}, scelti in modo da garantire la replicazione e la disponibilità dei dati.
In ogni nodo, una tabella può trovarsi in uno dei tre stati: preparata, creata o cancellata.
Le tabelle possono solo progredire di stato, nell'ordine: preparata $\rightarrow$ creata $\rightarrow$ cancellata.
Una tabella si considera creata quando tutti i \texttt{nodi responsabili} la considerano tale; si considera cancellata quando almeno un nodo la contrassegna come tale.
Lo stato di una tabella viene sincronizzato tra i nodi tramite il protocollo di Gossip (vedi sezione~\ref{subsec:gossip}).

\subsubsection{Creazione e Cancellazione}
\label{subsubsec:creazione-cancellazione}

Per creare una tabella, si seguono i seguenti passaggi:
\begin{enumerate}
    \item Il client si connette a un nodo a sua scelta (detto \texttt{router}) e invia i parametri \texttt{b}, \texttt{n}, \texttt{r} e \texttt{w} della tabella.
    \item Il \texttt{router} genera un UUID per la tabella e calcola la banda effettiva $B = b \cdot n$, per tenere conto della replicazione.
    \item Il \texttt{router} richiede agli altri nodi la banda disponibile per la nuova tabella, evitando di allocare più di \texttt{b} su un singolo nodo, garantendo così che la tabella venga replicata esattamente \texttt{n} volte.
    \item Una volta ottenuta sufficiente banda (almeno $B$), il \texttt{router} definisce l'elenco dei \texttt{nodi responsabili} e le rispettive quote.
    \item Il \texttt{router} richiede ai \texttt{nodi responsabili} di eseguire il \texttt{commit} della tabella.
    \item Se tutti i nodi effettuano il \texttt{commit}, la creazione ha successo e il \texttt{router} restituisce al client l'UUID della nuova tabella.
\end{enumerate}

%Lo step 3 è detto preparazione, mentre lo step 5 è detto commit.

Se un nodo riceve una richiesta di preparazione per una nuova tabella, ma non riceve un \texttt{commit} entro un certo timeout, segnerà la tabella come cancellata.
Questo garantisce che, in caso di errori durante la creazione, la tabella non rimanga in uno stato indefinito che comporterebbe perdita di banda allocata.

Per cancellare una tabella, è sufficiente che un nodo la contrassegni come cancellata; successivamente, grazie al protocollo di Gossip (vedi sezione~\ref{subsec:gossip}), gli altri nodi si sincronizzeranno e la tabella sarà considerata cancellata da tutti.

\subsubsection{Metadati}
\label{subsubsec:metadati}

Il database contiene una tabella speciale, non accessibile dagli utenti, identificata da uno UUIDv8 generato dai byte \texttt{kitsuraimetadata}.
Questa tabella contiene i metadati di tutte le tabelle create nel cluster, inclusi i relativi parametri e i \texttt{nodi responsabili}.
Grazie a ciò, ogni nodo può comportarsi come \texttt{router} nelle richieste di \texttt{get} e \texttt{set}.
La tabella dei metadati, identificata da uno UUIDv8, non può entrare in conflitto con gli UUIDv7 generati per le tabelle degli utenti.

\subsection{Gossip}
\label{subsec:gossip}

Ogni nodo del cluster avvia uno scambio di gossip con un peer scelto casualmente ogni secondo.
Durante lo scambio, i due nodi sincronizzano i metadati delle tabelle create e cancellate, ma non di quelle in preparazione.
L'utilizzo di un Merkle tree consente ai nodi di identificare rapidamente le tabelle divergenti e sincronizzarsi solo su quelle.
Se il peer non è disponibile, lo scambio fallisce.
Il gossip permette al sistema di raggiungere la coerenza dei metadati tra i nodi anche in presenza di nodi offline o errori di rete.

\subsection{Coppie Chiave-Valore}
\label{subsec:coppie-chiave-valore}

Each key in a table is assigned to `n` nodes from those responsible for the table,
respecting the ratio of allocated bandwidth on the nodes.

To do so, we first call `b_i` the bandwidth of node `i` and `Σ` the total allocated bandwidth.
We define a ring of size `Σ` and for each position in the ring we assign a node `i` such that:
- every `n`-long window in the ring contains distinct nodes;
- every node `i` is assigned to `b_i` positions in the ring.

Given a key we compute its hash (with [XXH3](https://xxhash.com) a fast non-crypto hash algorithm)
 and find the position in the ring.
The key is assigned to the `n` nodes to the right of the key's position in the ring.

Nodes are assigned to the ring iteratively, starting from the first node.
Each node is assigned to `b_i` positions starting from the last assigned and taking a slot every `n`-th position.
If the slot is already taken by another node, we skip it and take its successor.

As the allocated bandwidth could be considerably high the construction of a ring 
 that meets the above criteria could be expensive.
This construction can be avoided by using a formula
 to find which node is assigned to a given position in the ring,
 see [virt_to_peer](https://virv12.github.io/kitsurai/ktd/state/struct.TableData.html#method.virt_to_peer).

To read (or write) a key each of the `n` nodes is queried and the responses aggregated.
The first `r` (or `w`) responses available are returned to the client.

\subsubsection{Availability Zones}
\label{subsubsec:availability-zones}

To guarantee the database's availability each node can be tagged with a string, 
 identifying its availability zone, AZ from now.
To allocate each table's key in `n` AZs the previous allocation algorithm requires only a slight change:
- the router shall not take more than `b` bandwidth from nodes in a single AZ;
- nodes should be assigned in the ring ordered by their AZ.

\subsection{Remote Procedure Calls}
\label{subsec:rpc}

Remote procedures are executed with the following protocol:
1. A opens a TCP connection to B;
2. A sends the serialized RPC request;
3. A closes its TCP stream (sending `FIN` to B);
4. B waits until it has received all the available data;
5. B deserializes and executes the RPC;
6. B sends the RPC's response back and closes the connection;
7. A waits until it has received all of the response and returns the value to the callee.

## Future Features and Limitations
The current implementation does not provide any method to:
- resolve read conflicts;
- choose table names.

Both of these are intentionally not implemented as locking a solution to one of these problems
 requires strengthening the assumptions and increases the code complexity.

Nonetheless, both of these features could be implemented on top of the current interface as HTTP proxies.
To resolve read conflicts a proxy could inject in every write a timestamp 
 and on reads pick the latest version available to both update the out of date nodes and return to the client.  
For the latter another proxy could use the database itself as a storage for table names or an external database,
 to translate any query's table on the fly.

Lastly physical disks are not accounted and the system 
 could be generalized to support multiple bandwidths on a single node.

A limitation of the table allocation algorithm is its incapacity to manage the nodes' capacity.
 A table with little bandwidth requirements but large capacity needs could potentially brake the system.
A mitigation for this problem could change the way the bandwidth's unit to represent
 some ratio between available capacity and actual speed, coupled with a limitation in key number and sizes.

Currently, the cluster cannot change its nodes while it is running, 
 nor can a table migrate should a node persistently fail.
It should be possible to add both of these features 
 where some way to migrate to be implemented.  

## Benchmarks

The benchmarks are run on three instances on the same machine using the `bench.sh` script.
Below are the results on a log/log scale for the "item get" operation and the "item set" operation.

![](./benchmarks/bench-get.svg)
![](./benchmarks/bench-set.svg)
