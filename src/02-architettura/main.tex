\section{Architettura}
\label{sec:architettura}

Un sistema Kitsurai opera su un cluster, composto da un insieme statico di nodi, i quali possono risultare temporaneamente non disponibili.
Il sistema organizza i dati in tabelle, ognuna identificata da un UUIDv7. Ogni tabella contiene coppie chiave-valore, dove sia le chiavi che i valori sono array di byte.

\subsection{Interfaccia}
\label{subsec:interfaccia}

Sono supportate quattro operazioni principali:
\begin{itemize}
    \item \texttt{create-table}: crea una nuova tabella con i parametri specificati.
    \item \texttt{delete-table}: cancella una tabella esistente.
    \item \texttt{get}: recupera il valore associato a una chiave in una tabella.
    \item \texttt{set}: imposta il valore associato a una chiave in una tabella.
\end{itemize}

\subsection{Tabelle}
\label{subsec:tabelle}

Ogni tabella ha quattro parametri, specificati dall'utente al momento della creazione:
\begin{itemize}
    \item \texttt{bandwidth} o \texttt{b}: il numero di richieste al secondo che il sistema deve garantire per la tabella.
    \item \texttt{n}: il fattore di replicazione per ogni coppia chiave-valore.
    \item \texttt{r}: il \texttt{read-concern}, ovvero il numero minimo di nodi che devono rispondere con successo a una richiesta di lettura.
    \item \texttt{w}: il \texttt{write-concern}, ovvero il numero minimo di nodi che devono rispondere con successo a una richiesta di scrittura.
\end{itemize}

Ogni tabella è gestita da un insieme di nodi, chiamati \texttt{nodi responsabili}, scelti in modo da garantire la replicazione e la disponibilità dei dati.
In ogni nodo, una tabella può trovarsi in uno dei tre stati: preparata, creata o cancellata.
Le tabelle possono solo progredire di stato, nell'ordine: preparata $\rightarrow$ creata $\rightarrow$ cancellata.
Una tabella si considera creata quando tutti i \texttt{nodi responsabili} la considerano tale; si considera cancellata quando almeno un nodo la contrassegna come tale.
Lo stato di una tabella viene sincronizzato tra i nodi tramite il protocollo di Gossip (vedi sezione~\ref{subsec:gossip}).

\subsubsection{Creazione e Cancellazione}
\label{subsubsec:creazione-cancellazione}

Per creare una tabella, si seguono i seguenti passaggi:
\begin{enumerate}
    \item Il client si connette a un nodo a sua scelta (detto \texttt{router}) e invia i parametri \texttt{b}, \texttt{n}, \texttt{r} e \texttt{w} della tabella.
    \item Il \texttt{router} genera un UUID per la tabella e calcola la banda effettiva $B = b \cdot n$, per tenere conto della replicazione.
    \item Il \texttt{router} richiede agli altri nodi la banda disponibile per la nuova tabella, evitando di allocare più di \texttt{b} su un singolo nodo, garantendo così che la tabella venga replicata esattamente \texttt{n} volte.
    \item Una volta ottenuta sufficiente banda (almeno $B$), il \texttt{router} definisce l'elenco dei \texttt{nodi responsabili} e le rispettive quote.
    \item Il \texttt{router} richiede ai \texttt{nodi responsabili} di eseguire il \texttt{commit} della tabella.
    \item Se tutti i nodi effettuano il \texttt{commit}, la creazione ha successo e il \texttt{router} restituisce al client l'UUID della nuova tabella.
\end{enumerate}

%Lo step 3 è detto preparazione, mentre lo step 5 è detto commit.

Se un nodo riceve una richiesta di preparazione per una nuova tabella, ma non riceve un \texttt{commit} entro un certo timeout, segnerà la tabella come cancellata.
Questo garantisce che, in caso di errori durante la creazione, la tabella non rimanga in uno stato indefinito che comporterebbe perdita di banda allocata.

Per cancellare una tabella, è sufficiente che un nodo la contrassegni come cancellata; successivamente, grazie al protocollo di Gossip (vedi sezione~\ref{subsec:gossip}), gli altri nodi si sincronizzeranno e la tabella sarà considerata cancellata da tutti.

\subsubsection{Metadati}
\label{subsubsec:metadati}

Il database contiene una tabella speciale, non accessibile dagli utenti, identificata da uno UUIDv8 generato dai byte \texttt{kitsuraimetadata}.
Questa tabella contiene i metadati di tutte le tabelle create nel cluster, inclusi i relativi parametri e i \texttt{nodi responsabili}.
Grazie a ciò, ogni nodo può comportarsi come \texttt{router} nelle richieste di \texttt{get} e \texttt{set}.
La tabella dei metadati, identificata da uno UUIDv8, non può entrare in conflitto con gli UUIDv7 generati per le tabelle degli utenti.

\subsection{Gossip}
\label{subsec:gossip}

Ogni nodo del cluster avvia uno scambio di gossip con un peer scelto casualmente ogni secondo.
Durante lo scambio, i due nodi sincronizzano i metadati delle tabelle create e cancellate, ma non di quelle in preparazione.
L'utilizzo di un Merkle tree consente ai nodi di identificare rapidamente le tabelle divergenti e sincronizzarsi solo su quelle.
Se il peer non è disponibile, lo scambio fallisce.
Il gossip permette al sistema di raggiungere la coerenza dei metadati tra i nodi anche in presenza di nodi offline o errori di rete.

\subsection{Coppie Chiave-Valore}
\label{subsec:coppie-chiave-valore}

Ogni chiave della tabella è assegnata a \texttt{n} nodi tra quelli responsabili della tabella stessa, rispettando il rapporto di banda allocata sui nodi.
Per fare ciò, definiamo $b_i$ come la banda del nodo $i$ e $\Sigma$ come la banda totale allocata.
Definiamo un anello di dimensione $\Sigma$ e per ogni posizione nell'anello assegniamo un nodo $i$ in modo che:
\begin{itemize}
    \item ogni finestra di lunghezza $n$ nell'anello contenga nodi distinti;
    \item ogni nodo $i$ sia assegnato a $b_i$ posizioni nell'anello.
\end{itemize}

Data una chiave, calcoliamo il suo hash (utilizzando XXH3\footnote{https://xxhash.com}, un algoritmo di hash veloce non crittografico) e troviamo la posizione nell'anello.
La chiave viene assegnata ai \texttt{n} nodi a destra della posizione della chiave nell'anello.

I nodi sono assegnati all'anello in modo iterativo, partendo dal primo nodo.
Ogni nodo viene assegnato a $b_i$ posizioni, partendo dall'ultima assegnata e prendendo uno slot ogni $n$-esima posizione.
Se lo slot è già occupato da un altro nodo, si salta e si prende il successivo.

Poiché la banda allocata può essere considerevolmente alta, la costruzione di un anello che soddisfi i criteri sopra descritti può essere costosa.
Questa costruzione può essere evitata utilizzando una formula per trovare quale nodo è assegnato a una data posizione nell'anello, come descritto in \texttt{virt\_to\_peer} (vedi \url{https://virv12.github.io/kitsurai/ktd/state/struct.TableData.html#method.virt_to_peer}).

Per leggere o scrivere una chiave, si seguono i seguenti passaggi:
\begin{enumerate}
    \item Il client si connette a un nodo a sua scelta (detto \texttt{router}) e invia la chiave e il valore (se si tratta di una scrittura).
    \item Il \texttt{router} calcola l'hash della chiave e determina i \texttt{n} nodi responsabili per quella chiave.
    \item Il \texttt{router} invia la richiesta di lettura o scrittura ai nodi responsabili.
    \item I nodi rispondono con il valore richiesto o confermano la scrittura.
    \item Il \texttt{router} aggrega le risposte e restituisce al client i primi $r$ (o $w$) risultati disponibili.
\end{enumerate}

Se meno di $r$ (o $w$) nodi rispondono con successo, l'operazione fallisce e il client riceve un errore.

\subsubsection{Availability Zones}
\label{subsubsec:availability-zones}

Per garantire la disponibilità del database, ogni nodo può essere etichettato con una stringa che identifica la sua Availability Zone (\texttt{AZ}).
Due nodi appartenenti alla stessa \texttt{AZ} non possono essere scelti per replicare la stessa chiave.
Per garantire che le chiavi siano replicate in modo da essere disponibili anche in caso di guasti a un'intera \texttt{AZ}, andiamo a cambiare l'algoritmo di allocazione delle chiavi:
\begin{itemize}
    \item durante la creazione della tabella il \texttt{router} non deve prendere più di \texttt{b} banda da nodi nella stessa \texttt{AZ};
    \item i nodi devono essere assegnati all'anello in ordine di \texttt{AZ}.
\end{itemize}

\subsection{Remote Procedure Calls}
\label{subsec:rpc}

Kitsurai utilizza un protocollo di Remote Procedure Call (RPC) per comunicare tra i nodi del cluster.
Una richiesta RPC si svolge nel seguente modo:
\begin{enumerate}
    \item $A$ apre una connessione TCP a $B$.
    \item $A$ invia la richiesta RPC serializzata.
    \item $A$ chiude il suo stream TCP (inviando un pacchetto `FIN` a $B$).
    \item $B$ attende di ricevere tutti i dati disponibili.
    \item $B$ deserializza ed esegue la richiesta RPC.
    \item $B$ invia la risposta RPC serializzata e chiude la connessione.
    \item $A$ attende di ricevere tutti i dati della risposta e restituisce il valore al chiamante.
\end{enumerate}

\subsection{Limitazione del Throughput}
\label{subsec:limitazione-throughput}

In un sistema multi-tenant come Kitsurai è necessario evitare che un singolo client monopolizzi le risorse e comprometta le prestazioni degli altri.
Per questo motivo, Kitsurai implementa un meccanismo di limitazione del throughput che garantisce a ogni client la banda richiesta al momento della creazione della tabella, anche in presenza di carichi elevati e concorrenti.
Ogni nodo del cluster mantiene, per ciascuna tabella, una variabile \texttt{sched\_at} che indica il momento in cui il nodo sarà disponibile per eseguire nuove operazioni su quella tabella.
Quando un client invia una richiesta di lettura o scrittura, il nodo attende fino al tempo \texttt{sched\_at} prima di eseguire l'operazione, quindi incrementa \texttt{sched\_at} di un intervallo pari a $\frac{1}{b}$, dove $b$ è la banda richiesta per la tabella.

